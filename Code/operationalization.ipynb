{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Model operationalization & Deployment\n",
    "\n",
    "The best model is saved as a .model file along with the relevant scheme for deployment. The functions are first tested locally before operationalizing the model using Azure Machine Learning Model Management environment for use in production in realtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Setup our environment by importing required libraries\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "\n",
    "import glob\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.blob import PublicAccess\n",
    "\n",
    "# For creating pipelines and model\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Setup the pyspark environment\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE creating a local directory!\n",
      "+---------+--------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+-----------------+-----------------+-----------------+-----------------+------+---+-------------+--------+-------+\n",
      "|machineID|        dt_truncated|volt_rollingmean_3|rotate_rollingmean_3|pressure_rollingmean_3|vibration_rollingmean_3|volt_rollingmean_24|rotate_rollingmean_24|pressure_rollingmean_24|vibration_rollingmean_24| volt_rollingstd_3|rotate_rollingstd_3|pressure_rollingstd_3|vibration_rollingstd_3|volt_rollingstd_24|rotate_rollingstd_24|pressure_rollingstd_24|vibration_rollingstd_24|error1sum_rollingmean_24|error2sum_rollingmean_24|error3sum_rollingmean_24|error4sum_rollingmean_24|error5sum_rollingmean_24|         comp1sum|         comp2sum|         comp3sum|         comp4sum| model|age|model_encoded|failure1|label_e|\n",
      "+---------+--------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+-----------------+-----------------+-----------------+-----------------+------+---+-------------+--------+-------+\n",
      "|      114|2016-01-01 06:00:...| 163.3757329023745|  333.14948458556535|    100.18395169796328|     44.095881263819514| 164.11472399129667|     277.191815231867|      97.62891107072754|       50.88535051605059|21.004956521854176|  67.52872593782637|   12.936152686125933|     4.613597609179632|15.537773806210083|   67.65198854409817|     10.52827463297973|      6.941294875549091|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|            489.0|            549.0|            549.0|            564.0|model1| 18|    (3,[],[])|     0.0|    0.0|\n",
      "|      114|2016-01-01 03:00:...| 168.0560723838507|  279.96957534682707|     97.63372820912399|      48.24221679536314| 164.03977789009616|    265.4140301347857|       98.2692713270434|      51.574366655467436|17.820643824484396| 49.637429003930606|    9.789304756066265|     7.963344413644408| 15.48856269342518|   67.26398825199145|    10.343146088525474|      6.738109627670492|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|            489.0|            549.0|            549.0|            564.0|model1| 18|    (3,[],[])|     0.0|    0.0|\n",
      "|      114|2016-01-01 00:00:...|161.59623278663096|   277.7171441565807|     95.71420759202182|     51.508182534644625| 164.56778047540993|   265.99342004784285|       99.6572121485503|       50.46248490865013|20.582267478545578|  80.97739710207937|    8.128630498837182|     10.04603679663133|15.551165354528889|   65.23789560931164|    11.490148490289625|     7.7457818689251345|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|488.6666666666667|548.6666666666666|548.6666666666666|563.6666666666666|model1| 18|    (3,[],[])|     0.0|    0.0|\n",
      "|      114|2015-12-31 21:00:...| 170.0141386667267|   292.4471663647502|      86.0653955365305|     50.314535427527936| 165.41723263438084|   278.12231772796457|      98.87594592461198|      49.100686634977116|13.812544186838844| 56.224636185244115|    9.605933766887967|     6.294693994764171|15.402539242532207|   78.42674708913566|    12.387032843966068|      7.822982220380671|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|            488.0|            548.0|            548.0|            563.0|model1| 18|    (3,[],[])|     0.0|    0.0|\n",
      "|      114|2015-12-31 18:00:...|170.42359844411342|   334.3696511783739|     99.07688677874042|     48.853143551499244|  166.3251929489545|    286.4962812101407|     101.27560815245654|      47.541499460975025|  8.73668143235762|  44.16549629157097|    5.827054526494244|     6.890586104508928|15.088084779349089|   81.86613174940224|    11.384795095243218|      8.422065747978259|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|            488.0|            548.0|            548.0|            563.0|model1| 18|    (3,[],[])|     0.0|    0.0|\n",
      "+---------+--------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+-----------------+-----------------+-----------------+-----------------+------+---+-------------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Feature engineering final dataset files loaded!\n",
      "CPU times: user 8.76 s, sys: 2.67 s, total: 11.4 s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load the previous created final dataset into the workspace\n",
    "from azure.storage.blob import BlockBlobService\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# define parameters \n",
    "ACCOUNT_NAME = \"pdmvienna\"\n",
    "ACCOUNT_KEY = \"PDuXK61GpmMVWMrWdvr29THbPdlOXa61fN5RfgQV/jBO8berC1zLzZ678Nxrx+D3CRp4+ZvSff9al+lrUh8qUQ==\"\n",
    "CONTAINER_NAME = \"featureengineering\"\n",
    "\n",
    "# define your blob service     \n",
    "my_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n",
    "\n",
    "# create a local path where to store the results later.\n",
    "LOCAL_DIRECT = 'model_operationalize.parquet'\n",
    "if not os.path.exists(LOCAL_DIRECT):\n",
    "    os.makedirs(LOCAL_DIRECT)\n",
    "    print('DONE creating a local directory!')\n",
    "\n",
    "# define your blob service     \n",
    "my_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n",
    "\n",
    "# download the entire parquet result folder to local path for a new run \n",
    "for blob in my_service.list_blobs(CONTAINER_NAME):\n",
    "    if 'featureengineering_files.parquet' in blob.name:\n",
    "        local_file = os.path.join(LOCAL_DIRECT, os.path.basename(blob.name))\n",
    "        my_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n",
    "\n",
    "data = spark.read.parquet('model_operationalize.parquet')\n",
    "#data.persist()\n",
    "data.show(5)\n",
    "print('Feature engineering final dataset files loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the features, labels for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define list of input columns for downstream modeling - note model variable was removed as string was not supported\n",
    "input_features = [\n",
    "'volt_rollingmean_3',\n",
    "'rotate_rollingmean_3',\n",
    "'pressure_rollingmean_3',\n",
    "'vibration_rollingmean_3',\n",
    "'volt_rollingmean_24',\n",
    "'rotate_rollingmean_24',\n",
    "'pressure_rollingmean_24',\n",
    "'vibration_rollingmean_24',\n",
    "'volt_rollingstd_3',\n",
    "'rotate_rollingstd_3',\n",
    "'pressure_rollingstd_3',\n",
    "'vibration_rollingstd_3',\n",
    "'volt_rollingstd_24',\n",
    "'rotate_rollingstd_24',\n",
    "'pressure_rollingstd_24',\n",
    "'vibration_rollingstd_24',\n",
    "'error1sum_rollingmean_24',\n",
    "'error2sum_rollingmean_24',\n",
    "'error3sum_rollingmean_24',\n",
    "'error4sum_rollingmean_24',\n",
    "'error5sum_rollingmean_24',\n",
    "'comp1sum',\n",
    "'comp2sum',\n",
    "'comp3sum',\n",
    "'comp4sum',\n",
    "'age'  \n",
    "]\n",
    "\n",
    "label_var = ['label_e']\n",
    "key_cols =['machineID','dt_truncated']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assemble features\n",
    "va = VectorAssembler(inputCols=(input_features), outputCol='features')\n",
    "data = va.transform(data).select('machineID','dt_truncated','label_e','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set maxCategories so features with > 10 distinct values are treated as continuous.\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \n",
    "                               outputCol=\"indexedFeatures\", \n",
    "                               maxCategories=10).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit on whole dataset to include all labels in index\n",
    "labelIndexer = StringIndexer(inputCol=\"label_e\", outputCol=\"indexedLabel\").fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2174000\n",
      "747000\n"
     ]
    }
   ],
   "source": [
    "# split the data into train/test based on date\n",
    "training = data.filter(data.dt_truncated > \"2015-01-01\").filter(data.dt_truncated < \"2015-09-30\")\n",
    "testing = data.filter(data.dt_truncated > \"2015-09-30\")\n",
    "\n",
    "print(training.count())\n",
    "print(testing.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "# chain indexers and forest in a Pipeline\n",
    "pipeline_rf = Pipeline(stages=[labelIndexer, featureIndexer, rf])\n",
    "\n",
    "# train model.  This also runs the indexers.\n",
    "model_rf = pipeline_rf.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make predictions.\n",
    "predictions_rf = model_rf.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictionAndLabels = predictions_rf.select(\"indexedLabel\", \"prediction\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.994112\n"
     ]
    }
   ],
   "source": [
    "# select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\")\n",
    "print(\"Accuracy = %g\" % evaluator.evaluate(predictions_rf, {evaluator.metricName: \"accuracy\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a model that performs well, you can package it into a scoring service. To prepare for this, save your model and dataset schema locally first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model_rf.write().overwrite().save(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']+'pmdrfull.model')\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pmdrfull.model\r\n"
     ]
    }
   ],
   "source": [
    "# check to see if the model was saved in the shared location\n",
    "!ls $AZUREML_NATIVE_SHARE_DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authoring Realtime Web Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we show you how to author a realtime web service that scores the model you saved above. First check to ensure that the latest version of the azure-ml-api-sdk is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/home/mmlspark/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/home/mmlspark/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting azure-ml-api-sdk\n",
      "  Downloading azure_ml_api_sdk-0.1.0a11-py2.py3-none-any.whl (80kB)\n",
      "\u001b[K    100% |################################| 81kB 368kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz in /home/mmlspark/lib/conda/lib/python3.6/site-packages (from azure-ml-api-sdk)\n",
      "Collecting liac-arff (from azure-ml-api-sdk)\n",
      "  Downloading liac-arff-2.1.1.tar.gz\n",
      "Requirement already satisfied: python-dateutil in /home/mmlspark/lib/conda/lib/python3.6/site-packages (from azure-ml-api-sdk)\n",
      "Requirement already satisfied: six>=1.5 in /home/mmlspark/lib/conda/lib/python3.6/site-packages (from python-dateutil->azure-ml-api-sdk)\n",
      "Installing collected packages: liac-arff, azure-ml-api-sdk\n",
      "  Running setup.py install for liac-arff ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed azure-ml-api-sdk-0.1.0a11 liac-arff-2.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-ml-api-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from azure.ml.api.schema.dataTypes import DataTypes\n",
    "from azure.ml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azure.ml.api.realtime.services import generate_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define init and run functions\n",
    "We start by defining our init and run functions in the cell below. Then write them to the score.py file. This file will load the model, perform the prediction, and return the result.\n",
    "The init function initializes your web service, loading in any data or models that you need to score your inputs. In the example below, we load in the trained model. This command is run when the Docker container containing your service initializes.\n",
    "The run function defines what is executed on a scoring call. In our simple example, we simply load in the input as a data frame, and run our pipeline on the input, and return the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    # read in the model file\n",
    "    from pyspark.ml import PipelineModel\n",
    "    global pipeline\n",
    "    \n",
    "    pipeline = PipelineModel.load(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']+'pmdrfull.model')\n",
    "    \n",
    "def run(input_df):\n",
    "    import json\n",
    "    response = ''\n",
    "    try:\n",
    "        #Get prediction results for the dataframe\n",
    "        input_features = [\n",
    "            'volt_rollingmean_3',\n",
    "            'rotate_rollingmean_3',\n",
    "            'pressure_rollingmean_3',\n",
    "            'vibration_rollingmean_3',\n",
    "            'volt_rollingmean_24',\n",
    "            'rotate_rollingmean_24',\n",
    "            'pressure_rollingmean_24',\n",
    "            'vibration_rollingmean_24',\n",
    "            'volt_rollingstd_3',\n",
    "            'rotate_rollingstd_3',\n",
    "            'pressure_rollingstd_3',\n",
    "            'vibration_rollingstd_3',\n",
    "            'volt_rollingstd_24',\n",
    "            'rotate_rollingstd_24',\n",
    "            'pressure_rollingstd_24',\n",
    "            'vibration_rollingstd_24',\n",
    "            'error1sum_rollingmean_24',\n",
    "            'error2sum_rollingmean_24',\n",
    "            'error3sum_rollingmean_24',\n",
    "            'error4sum_rollingmean_24',\n",
    "            'error5sum_rollingmean_24',\n",
    "            'comp1sum',\n",
    "            'comp2sum',\n",
    "            'comp3sum',\n",
    "            'comp4sum',\n",
    "            'age',\n",
    "        ]\n",
    "        \n",
    "        va = VectorAssembler(inputCols=(input_features), outputCol='features')\n",
    "        data = va.transform(input_df).select('machineID','features')\n",
    "        score = pipeline.transform(data)\n",
    "        predictions = score.collect()\n",
    "\n",
    "        #Get each scored result\n",
    "        preds = [str(x['prediction']) for x in predictions]\n",
    "        response = \",\".join(preds)\n",
    "    except Exception as e:\n",
    "        print(\"Error: {0}\",str(e))\n",
    "        return (str(e))\n",
    "    \n",
    "    # Return results\n",
    "    print(json.dumps(response))\n",
    "    return json.dumps(response)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create schema and schema file\n",
    "Create a schema for the input to the web service and generate the schema file. This will be used to create a Swagger file for your web service which can be used to discover its input and sample data when calling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the input data frame\n",
    "inputs = {\"input_df\": SampleDefinition(DataTypes.SPARK, data.drop(\"dt_truncated\",\"failure1\",\"label_e\", \"model\",\"model_encoded\"))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': {'input_df': {'internal': {'fields': [{'metadata': {}, 'type': 'integer', 'name': 'machineID', 'nullable': True}, {'metadata': {'ml_attr': {'num_attrs': 26, 'attrs': {'numeric': [{'idx': 0, 'name': 'volt_rollingmean_3'}, {'idx': 1, 'name': 'rotate_rollingmean_3'}, {'idx': 2, 'name': 'pressure_rollingmean_3'}, {'idx': 3, 'name': 'vibration_rollingmean_3'}, {'idx': 4, 'name': 'volt_rollingmean_24'}, {'idx': 5, 'name': 'rotate_rollingmean_24'}, {'idx': 6, 'name': 'pressure_rollingmean_24'}, {'idx': 7, 'name': 'vibration_rollingmean_24'}, {'idx': 8, 'name': 'volt_rollingstd_3'}, {'idx': 9, 'name': 'rotate_rollingstd_3'}, {'idx': 10, 'name': 'pressure_rollingstd_3'}, {'idx': 11, 'name': 'vibration_rollingstd_3'}, {'idx': 12, 'name': 'volt_rollingstd_24'}, {'idx': 13, 'name': 'rotate_rollingstd_24'}, {'idx': 14, 'name': 'pressure_rollingstd_24'}, {'idx': 15, 'name': 'vibration_rollingstd_24'}, {'idx': 16, 'name': 'error1sum_rollingmean_24'}, {'idx': 17, 'name': 'error2sum_rollingmean_24'}, {'idx': 18, 'name': 'error3sum_rollingmean_24'}, {'idx': 19, 'name': 'error4sum_rollingmean_24'}, {'idx': 20, 'name': 'error5sum_rollingmean_24'}, {'idx': 21, 'name': 'comp1sum'}, {'idx': 22, 'name': 'comp2sum'}, {'idx': 23, 'name': 'comp3sum'}, {'idx': 24, 'name': 'comp4sum'}, {'idx': 25, 'name': 'age'}]}}}, 'type': {'pyClass': 'pyspark.ml.linalg.VectorUDT', 'sqlType': {'fields': [{'metadata': {}, 'type': 'byte', 'name': 'type', 'nullable': False}, {'metadata': {}, 'type': 'integer', 'name': 'size', 'nullable': True}, {'metadata': {}, 'type': {'containsNull': False, 'elementType': 'integer', 'type': 'array'}, 'name': 'indices', 'nullable': True}, {'metadata': {}, 'type': {'containsNull': False, 'elementType': 'double', 'type': 'array'}, 'name': 'values', 'nullable': True}], 'type': 'struct'}, 'type': 'udt', 'class': 'org.apache.spark.ml.linalg.VectorUDT'}, 'name': 'features', 'nullable': True}], 'type': 'struct'}, 'type': 2, 'swagger': {'items': {'type': 'object', 'properties': {'features': {'type': 'object'}, 'machineID': {'format': 'int32', 'type': 'integer'}}}, 'type': 'array', 'example': [{'features': '[163.375732902,333.149484586,100.183951698,44.0958812638,164.114723991,277.191815232,97.6289110707,50.8853505161,21.0049565219,67.5287259378,12.9361526861,4.61359760918,15.5377738062,67.6519885441,10.528274633,6.94129487555,0.0,0.0,0.0,0.0,0.0,489.0,549.0,549.0,564.0,18.0]', 'machineID': 114}, {'features': '[168.056072384,279.969575347,97.6337282091,48.2422167954,164.03977789,265.414030135,98.269271327,51.5743666555,17.8206438245,49.6374290039,9.78930475607,7.96334441364,15.4885626934,67.263988252,10.3431460885,6.73810962767,0.0,0.0,0.0,0.0,0.0,489.0,549.0,549.0,564.0,18.0]', 'machineID': 114}, {'features': '[161.596232787,277.717144157,95.714207592,51.5081825346,164.567780475,265.993420048,99.6572121486,50.4624849087,20.5822674785,80.9773971021,8.12863049884,10.0460367966,15.5511653545,65.2378956093,11.4901484903,7.74578186893,0.0,0.0,0.0,0.0,0.0,488.666666667,548.666666667,548.666666667,563.666666667,18.0]', 'machineID': 114}]}}}}\n"
     ]
    }
   ],
   "source": [
    "x = generate_schema(run_func=run, inputs=inputs, filepath='service_schema.json')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test init and run\n",
    "We can then test the init and run functions right here in the notebook, before we decide to actually publish a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[114,\n",
       "  163.375732902,\n",
       "  333.149484586,\n",
       "  100.183951698,\n",
       "  44.0958812638,\n",
       "  164.114723991,\n",
       "  277.191815232,\n",
       "  97.6289110707,\n",
       "  50.8853505161,\n",
       "  21.0049565219,\n",
       "  67.5287259378,\n",
       "  12.9361526861,\n",
       "  4.61359760918,\n",
       "  15.5377738062,\n",
       "  67.6519885441,\n",
       "  10.528274633,\n",
       "  6.94129487555,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  489.0,\n",
       "  549.0,\n",
       "  549.0,\n",
       "  564.0,\n",
       "  18.0]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how the input data should be\n",
    "input_data = [[114, 163.375732902,333.149484586,100.183951698,44.0958812638,164.114723991,277.191815232,97.6289110707,50.8853505161,21.0049565219,67.5287259378,12.9361526861,4.61359760918,15.5377738062,67.6519885441,10.528274633,6.94129487555,0.0,0.0,0.0,0.0,0.0,489.0,549.0,549.0,564.0,18.0]]\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = (spark.createDataFrame(input_data, [\"machineID\", \"volt_rollingmean_3\", \"rotate_rollingmean_3\", \"pressure_rollingmean_3\", \"vibration_rollingmean_3\", \"volt_rollingmean_24\", \n",
    "            \"rotate_rollingmean_24\", \"pressure_rollingmean_24\", \"vibration_rollingmean_24\", \"volt_rollingstd_3\", \"rotate_rollingstd_3\",\n",
    "            \"pressure_rollingstd_3\", \"vibration_rollingstd_3\", \"volt_rollingstd_24\", \"rotate_rollingstd_24\", \"pressure_rollingstd_24\",\n",
    "            \"vibration_rollingstd_24\", \"error1sum_rollingmean_24\", \"error2sum_rollingmean_24\", \"error3sum_rollingmean_24\",\n",
    "            \"error4sum_rollingmean_24\", \"error5sum_rollingmean_24\", \"comp1sum\", \"comp2sum\", \"comp3sum\", \"comp4sum\",\n",
    "            \"age\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"0.0\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"0.0\"'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the schema file for deployment\n",
    "import json\n",
    "out = json.dumps(x)\n",
    "with open(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY'] + 'service_schema.json', 'w') as f:\n",
    "    f.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pmdrfull.model\tservice_schema.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls $AZUREML_NATIVE_SHARE_DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the user will need to navigate to the folder: \n",
    "```C:\\Users\\<username>\\.azureml\\share\\<team account>\\<Project Name> ```\n",
    "\n",
    "Copy the file service_schema.json to your projects folder for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use %%writefile command will save the *.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pdmscore.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pdmscore.py\n",
    "# after testing the below init() and run() functions,\n",
    "# uncomment this cell to create the score.py after.\n",
    "\n",
    "# remove import from init() from function.\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import concat, col, udf, lag, date_add, explode, lit, unix_timestamp\n",
    "from pyspark.sql.functions import month, weekofyear, dayofmonth\n",
    "from pyspark.sql.functions import datediff, to_date, lit, unix_timestamp\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.dataframe import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml.feature import StandardScaler, PCA, RFormula\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from azureml.api.schema.dataTypes import DataTypes\n",
    "from azureml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azureml.api.realtime.services import generate_schema\n",
    "\n",
    "\n",
    "def init():\n",
    "    # read in the model file\n",
    "    from pyspark.ml import PipelineModel\n",
    "    global pipeline\n",
    "    pipeline = PipelineModel.load(\"/azureml-share/pdmrfull.model\")\n",
    "    \n",
    "def run(input_df):\n",
    "    import json\n",
    "    response = ''\n",
    "    \n",
    "    try:\n",
    "        #Get prediction results for the dataframe\n",
    "        input_features = [\n",
    "            'volt_rollingmean_3',\n",
    "            'rotate_rollingmean_3',\n",
    "            'pressure_rollingmean_3',\n",
    "            'vibration_rollingmean_3',\n",
    "            'volt_rollingmean_24',\n",
    "            'rotate_rollingmean_24',\n",
    "            'pressure_rollingmean_24',\n",
    "            'vibration_rollingmean_24',\n",
    "            'volt_rollingstd_3',\n",
    "            'rotate_rollingstd_3',\n",
    "            'pressure_rollingstd_3',\n",
    "            'vibration_rollingstd_3',\n",
    "            'volt_rollingstd_24',\n",
    "            'rotate_rollingstd_24',\n",
    "            'pressure_rollingstd_24',\n",
    "            'vibration_rollingstd_24',\n",
    "            'error1sum_rollingmean_24',\n",
    "            'error2sum_rollingmean_24',\n",
    "            'error3sum_rollingmean_24',\n",
    "            'error4sum_rollingmean_24',\n",
    "            'error5sum_rollingmean_24',\n",
    "            'comp1sum',\n",
    "            'comp2sum',\n",
    "            'comp3sum',\n",
    "            'comp4sum',\n",
    "            'age',\n",
    "        ]\n",
    "\n",
    "        va = VectorAssembler(inputCols=(input_features), outputCol='features')\n",
    "        data = va.transform(input_df).select('machineID','features')\n",
    "        score = pipeline.transform(data)\n",
    "        predictions = score.collect()\n",
    "\n",
    "        #Get each scored result\n",
    "        preds = [str(x['prediction']) for x in predictions]\n",
    "        response = \",\".join(preds)\n",
    "    except Exception as e:\n",
    "        print(\"Error: {0}\",str(e))\n",
    "        return (str(e))\n",
    "    \n",
    "    # Return results\n",
    "    print(json.dumps(response))\n",
    "    return json.dumps(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    init()\n",
    "    run(\"{\\\"input_df\\\":[{\\\"machineID\\\":114,\\\"volt_rollingmean_3\\\":163.375732902,\\\"rotate_rollingmean_3\\\":333.149484586,\\\"pressure_rollingmean_3\\\":100.183951698,\\\"vibration_rollingmean_3\\\":44.0958812638,\\\"volt_rollingmean_24\\\":164.114723991,\\\"rotate_rollingmean_24\\\":277.191815232,\\\"pressure_rollingmean_24\\\":97.6289110707,\\\"vibration_rollingmean_24\\\":50.8853505161,\\\"volt_rollingstd_3\\\":21.0049565219,\\\"rotate_rollingstd_3\\\":67.5287259378,\\\"pressure_rollingstd_3\\\":12.9361526861,\\\"vibration_rollingstd_3\\\":4.61359760918,\\\"volt_rollingstd_24\\\":15.5377738062,\\\"rotate_rollingstd_24\\\":67.6519885441,\\\"pressure_rollingstd_24\\\":10.528274633,\\\"vibration_rollingstd_24\\\":6.94129487555,\\\"error1sum_rollingmean_24\\\":0.0,\\\"error2sum_rollingmean_24\\\":0.0,\\\"error3sum_rollingmean_24\\\":0.0,\\\"error4sum_rollingmean_24\\\":0.0,\\\"error5sum_rollingmean_24\\\":0.0,\\\"comp1sum\\\":489.0,\\\"comp2sum\\\":549.0,\\\"comp3sum\\\":549.0,\\\"comp4sum\\\":564.0,\\\"age\\\":18.0}]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the CLI to deploy and manage your web service \n",
    "\n",
    "## Pre-requisites \n",
    "\n",
    "Use the following commands to set up an environment and account to run the web service. For more info, see the Getting Started Guide and the CLI Command Reference. You can use -h flag at the end of the commands for command help.\n",
    "\n",
    "• Create the environment (you need to do this once per environment e.g. dev or prod)\n",
    "```\n",
    "az ml env setup -c -n <yourclustername> --location <e.g. eastus2>\n",
    "```\n",
    "\n",
    "• Create a Model Management account (one time setup)\n",
    "```\n",
    "az ml account modelmanagement create --location <e.g. eastus2> -n <your-new-acctname> -g <yourresourcegroupname> --sku-instances 1 --sku-name S1\n",
    "```\n",
    "\n",
    "• Set the Model Management account\n",
    "```\n",
    "az ml account modelmanagement set -n <youracctname> -g <yourresourcegroupname>\n",
    "```\n",
    "\n",
    "• Set the environment. The cluster name is the name used in step 1 above. The resource group name was the output of the same process and would be in the command window when the setup process is completed.\n",
    "```\n",
    "az ml env set -n <yourclustername> -g <yourresourcegroupname>\n",
    "```\n",
    "\n",
    "## Deploy your web service \n",
    "\n",
    "Switch to a bash shell, and run the following commands to deploy your service and run it.\n",
    "\n",
    "Enter the path where the notebook and other files are saved. Your actual path may be different from this example.\n",
    "```\n",
    "cd ~/notebooks/azureml/realtime/\n",
    "```\n",
    "\n",
    "This assumes that you saved your model locally.\n",
    "```\n",
    "az ml service create realtime -f pdmscore.py -r  spark-py -m pdmrfull.model -s service_schema.json -n pdmservice --cpu 0.1\n",
    "```\n",
    "\n",
    "This command will return the sample run command with sample data. You can get the Service Id from the output of the create command above.\n",
    "```\n",
    "az ml service show realtime -i <yourserviceid>\n",
    "```\n",
    "\n",
    "Call the web service to get a prediction\n",
    "```\n",
    "az ml service run realtime -i <yourserviceid> -d \"{\\\"input_df\\\": [{\\\"machineID\\\":114, \\\"vo\n",
    "lt_rollingmean_3\\\":163.375732902, \\\"rotate_rollingmean_3\\\":333.149484586, \\\"pressure_rollingmean_3\\\":100.183951698, \\\"vibration_rollingmean_3\\\":44.0958812638, \\\"volt_rollingme\n",
    "an_24\\\":164.114723991, \\\"rotate_rollingmean_24\\\":277.191815232, \\\"pressure_rollingmean_24\\\":97.6289110707, \\\"vibration_rollingmean_24\\\":50.8853505161, \\\"volt_rollingstd_3\\\":21\n",
    ".0049565219, \\\"rotate_rollingstd_3\\\":67.5287259378, \\\"pressure_rollingstd_3\\\":12.9361526861, \\\"vibration_rollingstd_3\\\":4.61359760918, \\\"volt_rollingstd_24\\\":15.5377738062, \\\"\n",
    "rotate_rollingstd_24\\\":67.6519885441, \\\"pressure_rollingstd_24\\\":10.528274633, \\\"vibration_rollingstd_24\\\":6.94129487555, \\\"error1sum_rollingmean_24\\\":0.0, \\\"error2sum_rolling\n",
    "mean_24\\\":0.0, \\\"error3sum_rollingmean_24\\\":0.0, \\\"error4sum_rollingmean_24\\\":0.0, \\\"error5sum_rollingmean_24\\\":0.0, \\\"comp1sum\\\":489.0, \\\"comp2sum\\\":549.0, \\\"comp3sum\\\":549.0\n",
    ", \\\"comp4sum\\\":564.0, \\\"age\\\":180}]}\"\n",
    "```\n",
    "\n",
    "Predicted output label is as follows:\n",
    "```\n",
    "\"0.0\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdm_release docker",
   "language": "python",
   "name": "pdm_release_docker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
